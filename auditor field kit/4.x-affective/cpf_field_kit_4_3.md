# üìã CPF FIELD KIT 4.3: TRUST TRANSFERENCE TO SYSTEMS

## ‚ö° QUICK ASSESSMENT (5 minutes)

**Check each item based on direct observation or immediate client response:**

‚ñ° **Q1**: Do employees routinely approve system-generated requests without manual verification?
   - *Evidence: Ask to see last 5 automated approvals*

‚ñ° **Q2**: When systems produce errors, are they typically blamed on user error or external factors?
   - *Evidence: Review recent incident reports*

‚ñ° **Q3**: Do security alerts from "trusted" systems get acted upon without authentication verification?
   - *Evidence: Ask about last security alert response*

‚ñ° **Q4**: Are AI/automated recommendations followed without human oversight for important decisions?
   - *Evidence: Request decision audit trail*

‚ñ° **Q5**: Is there formal training specifically about system limitations and verification requirements?
   - *Evidence: Check training records and materials*

‚ñ° **Q6**: Do employees use anthropomorphic language when describing business systems?
   - *Evidence: Listen during interviews for "the system knows/thinks/decides"*

‚ñ° **Q7**: Are there technical controls requiring human verification for high-impact system actions?
   - *Evidence: Test approval workflows*

---

## üìù EVIDENCE COLLECTION (10 minutes)

### Documents to Request
- [ ] **Recent incident reports** (last 6 months) - focus on system errors
- [ ] **Approval workflow documentation** - automated vs manual processes  
- [ ] **Security alert response logs** - last 3 months
- [ ] **Training materials** - system usage and limitations
- [ ] **System reliability reports** - error rates and downtime

### Demonstrations to Request
- [ ] **"Show me your last automated approval"** - walk through the process
- [ ] **"Demonstrate alert response"** - how you handle security notifications
- [ ] **"Show system override process"** - when humans override system recommendations

### System Checks to Perform
- [ ] **Test approval bypass** - can automated requests skip human verification?
- [ ] **Review alert settings** - which systems auto-trigger responses?
- [ ] **Check audit logs** - verification patterns in system interactions

### Key Interview Targets
- [ ] **System administrators** - daily system trust behaviors
- [ ] **Department managers** - decision-making with system outputs
- [ ] **Security team** - alert response procedures
- [ ] **End users** - routine system interactions

---

## üéØ RAPID SCORING (2 minutes)

### Decision Tree

**START HERE** ‚Üí Count "Yes" answers from Quick Assessment:

**If 0-2 "Yes" answers** ‚Üí **üü¢ GREEN (0)** - Strong System Skepticism
- Employees regularly verify system outputs
- Errors properly attributed to system limitations
- Human oversight maintained

**If 3-5 "Yes" answers** ‚Üí **üü° YELLOW (1)** - Selective System Trust  
- Some verification procedures in place
- Mixed responses to system errors
- Partial reliance on automated processes

**If 6-7 "Yes" answers** ‚Üí **üî¥ RED (2)** - High System Trust
- Minimal verification of system outputs
- System errors blamed on external factors
- Heavy reliance on automated decisions

---

## üîß SOLUTION PRIORITIES (5 minutes)

### HIGH IMPACT - Immediate Implementation

**üî¥ Critical (Complete within 30 days)**
- [ ] **Mandatory verification protocols** - *Cost: Low* - *Tech requirement: Policy + workflow*
- [ ] **Security alert authentication** - *Cost: Low* - *Tech requirement: Process change*

### MEDIUM IMPACT - Short-term Projects  

**üü° Important (Complete within 90 days)**
- [ ] **Trust-aware security training** - *Cost: Medium* - *Dependencies: Training team*
- [ ] **System reliability transparency** - *Cost: Medium* - *Dependencies: Monitoring tools*

### ONGOING PROGRAMS - Long-term Culture Change

**üü¢ Strategic (6-12 month initiatives)**
- [ ] **System output validation tools** - *Cost: High* - *Dependencies: Development resources*
- [ ] **Human-in-the-loop controls** - *Cost: High* - *Dependencies: System integration*

---

## üí¨ CLIENT CONVERSATION SCRIPT (3 minutes)

### Opening Questions
**"I'd like to understand how your team works with automated systems..."**

- "Walk me through your last important decision that involved system-generated data"
- "What happened the last time one of your core systems had an unexpected error?"
- "How do you handle approval requests that come from automated processes?"

### Follow-up Prompts for Incomplete Answers
- **If they say "we trust our systems"**: *"Can you show me the last time you verified a system output?"*
- **If they blame user error**: *"Tell me about the system's documented limitations"*  
- **If they mention AI tools**: *"How do you handle cases where AI recommendations conflict with human judgment?"*

### Red Flag Indicators Requiring Deeper Investigation
- **Defensive responses** about system reliability
- **Unable to recall system errors** (suggests attribution issues)
- **No training documentation** on system limitations
- **Anthropomorphic language** about systems ("the AI decided", "the system knows")

### Professional Language for Sensitive Topics
- Use: *"verification procedures"* instead of *"trust issues"*
- Use: *"system reliability assessment"* instead of *"your systems are unreliable"*
- Use: *"decision support processes"* instead of *"over-dependence on automation"*

---

## üìä FIELD NOTES TEMPLATE

### Assessment Summary
**Date**: _________________ **Auditor**: _______________________  
**Organization**: _________________________________ **Score**: ‚¨ú Green ‚¨ú Yellow ‚¨ú Red

### Evidence Collected
**Documents Reviewed**: 
‚ñ° Incident reports ‚ñ° Approval workflows ‚ñ° Alert logs ‚ñ° Training materials ‚ñ° Reliability reports

**Key Findings**:
- Verification rate for automated approvals: ______%
- Last system error attribution: ‚¨ú System limitation ‚¨ú User error ‚¨ú External factor
- Security training includes system limitations: ‚¨ú Yes ‚¨ú No ‚¨ú Partial

### Immediate Recommendations
**Priority 1** (30 days): ________________________________________________
**Priority 2** (90 days): ________________________________________________  
**Priority 3** (6-12 months): ___________________________________________

### Follow-up Required
‚ñ° Technical controls assessment  
‚ñ° Additional staff interviews  
‚ñ° System configuration review  
‚ñ° Training program evaluation

**Next Steps**: ___________________________________________________________

---

## ‚úÖ COMPLETION CHECKLIST

Assessment Complete When:
- [ ] All 7 quick assessment items checked
- [ ] Minimum 3 evidence sources collected  
- [ ] Score determined using decision tree
- [ ] Priority recommendations identified
- [ ] Field notes template completed
- [ ] Client debriefed with initial findings

**Total Time**: Should not exceed 25 minutes for core assessment