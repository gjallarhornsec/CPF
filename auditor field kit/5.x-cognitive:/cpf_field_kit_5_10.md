# üìã CPF INDICATOR 5.10 FIELD KIT
## Mental Model Confusion Assessment

---

## ‚ö° QUICK ASSESSMENT (5 minutes)

**Instructions**: Check YES/NO for each observable indicator. No interpretation required.

**1. Help Desk Spike After Changes**
- [ ] YES: Help desk tickets increase >30% within 30 days of security system changes
- [ ] NO: Help desk volume remains stable after system changes

**2. Training Update Process**
- [ ] YES: Organization has documented process for updating training when security systems change
- [ ] NO: No formal process or training updates happen inconsistently

**3. Multiple Security Tool Interfaces**
- [ ] YES: End users interact with 4+ different security tools daily with inconsistent interfaces
- [ ] NO: Users interact with <4 tools OR interfaces are standardized

**4. Incident Root Cause Pattern**
- [ ] YES: Security incidents regularly involve user misunderstanding of system behavior
- [ ] NO: Incidents rarely involve user confusion about how systems work

**5. User Understanding Validation**
- [ ] YES: Organization tests whether users understand security systems after changes
- [ ] NO: No formal testing of user understanding occurs

**6. Unofficial Procedure Variations**
- [ ] YES: Different teams have developed different unofficial procedures for same security tasks
- [ ] NO: Security procedures are consistent across teams

**7. System Change Communication**
- [ ] YES: Security implications are formally communicated when business systems change
- [ ] NO: Security implications are not systematically communicated during changes

**QUICK SCORE**: Count YES answers for questions 1, 3, 4, 6 + NO answers for questions 2, 5, 7 = ___/7

---

## üìù EVIDENCE COLLECTION (10 minutes)

### Documents to Request
**Priority 1 (Must Have)**:
- [ ] Help desk ticket logs from last 3 system changes
- [ ] Security training update procedures/documentation
- [ ] List of security tools end users interact with daily
- [ ] Recent security incident reports and root cause analyses

**Priority 2 (Nice to Have)**:
- [ ] User training materials for security systems
- [ ] Change management procedures
- [ ] Department-specific security procedures

### Demonstrations to Request
- [ ] **"Show me your process"**: How you update user training when security systems change
- [ ] **"Walk me through"**: Most recent security system change and training response
- [ ] **"Demonstrate"**: How different teams handle the same security requirement

### System Checks to Perform
- [ ] Count number of different security tools/interfaces users access
- [ ] Review help desk categories for "how do I..." or confusion queries
- [ ] Check for correlation between system changes and incident dates
- [ ] Verify existence of user comprehension testing procedures

### Interview Targets (15 minutes total)
- [ ] **Help Desk Manager** (5 min): Query patterns after system changes
- [ ] **Training Coordinator** (5 min): Process for updating security training
- [ ] **End User Representative** (5 min): Experience with security tool differences

---

## üéØ RAPID SCORING (2 minutes)

### Decision Tree
**Start Here**: Quick Assessment Score ___/7

**If Score 0-2**: ‚Üí **GREEN**
- Help desk stable after changes
- Training processes documented
- User understanding regularly tested

**If Score 3-4**: ‚Üí **YELLOW**
- Some confusion indicators present
- Moderate help desk increases
- Inconsistent training updates

**If Score 5-7**: ‚Üí **RED**
- Multiple confusion indicators
- High help desk volume after changes
- No systematic user understanding validation

### Objective Thresholds
- **Help Desk Increase**: >30% = Red Flag
- **Security Tools**: 4+ different interfaces = Red Flag  
- **Training Process**: Undocumented = Red Flag
- **Understanding Testing**: Never/Rarely = Red Flag

---

## üîß SOLUTION PRIORITIES (5 minutes)

### HIGH IMPACT / QUICK IMPLEMENTATION
- [ ] **Mental Model Update Protocol**
  - Cost: LOW | Time: 2-4 weeks
  - Create mandatory impact assessment for security changes
  - Require user comprehension testing before system access

- [ ] **Help Desk Query Categorization**
  - Cost: LOW | Time: 1 week
  - Tag confusion-related tickets to track patterns
  - Create early warning system for mental model gaps

### MEDIUM IMPACT / MEDIUM IMPLEMENTATION  
- [ ] **Interface Standardization Program**
  - Cost: MEDIUM | Time: 3-6 months
  - Standardize security tool interfaces where possible
  - Create overlay training for unavoidable differences

- [ ] **Cross-Team Procedure Alignment**
  - Cost: LOW | Time: 6-8 weeks
  - Quarterly sessions to align security procedures
  - Document and resolve team-specific variations

### HIGH IMPACT / LONG-TERM IMPLEMENTATION
- [ ] **Active Mental Model Testing**
  - Cost: MEDIUM | Time: 4-6 months
  - Deploy prediction exercises for security scenarios
  - Real-time mental model correction program

- [ ] **Simplified System Architecture**
  - Cost: HIGH | Time: 6-12 months
  - Consolidate security tools where feasible
  - Reduce cognitive complexity of security interactions

### Dependencies
- **Training Department**: Required for mental model update protocols
- **IT Architecture**: Needed for interface standardization
- **Help Desk System**: Must support ticket categorization
- **Change Management**: Integration with existing processes required

---

## üí¨ CLIENT CONVERSATION SCRIPT (3 minutes)

### Opening Questions
**"Tell me about your most recent security system change..."**
- How did you communicate changes to users?
- What kind of questions did you get afterward?
- How long did it take for help desk volume to return to normal?

### Follow-up Prompts
**If they mention training updates:**
- "Walk me through exactly how you updated the training materials"
- "How did you verify users understood the changes?"
- "What percentage of users completed the updated training?"

**If they mention help desk issues:**
- "What specific types of questions were users asking?"
- "How did you track and categorize these confusion-related tickets?"
- "Did any of these questions surprise you?"

**If they mention multiple security tools:**
- "How consistent are the interfaces across your security tools?"
- "Do users ever get confused about which procedure applies to which tool?"
- "Have you had incidents where users applied the wrong procedure?"

### Red Flag Indicators
- [ ] "Users figure it out eventually"
- [ ] "We don't really track that"
- [ ] "Each team does it their own way"
- [ ] "The tools are pretty intuitive"
- [ ] "We assume people know how to use them"

### Professional Language for Sensitive Topics
- **Instead of**: "Your users are confused"
- **Say**: "We're seeing some opportunities to align user expectations with system behavior"

- **Instead of**: "This is a major vulnerability"  
- **Say**: "This represents an area where we can strengthen your security posture"

- **Instead of**: "Your training is inadequate"
- **Say**: "There's an opportunity to enhance the connection between training and system changes"

---

## üìä FIELD NOTES TEMPLATE

### Client Information
**Organization**: _________________ **Date**: _________
**Primary Contact**: _________________ **Role**: _________

### Assessment Results
**Quick Assessment Score**: ___/7
**Risk Level**: GREEN / YELLOW / RED
**Primary Concerns**: 
- [ ] Help desk confusion spikes
- [ ] Inconsistent security procedures  
- [ ] No user understanding validation
- [ ] Multiple incompatible interfaces
- [ ] Other: ________________________

### Evidence Collected
**Documents Reviewed**: ‚òê Help Desk Logs ‚òê Training Procedures ‚òê Incident Reports ‚òê Tool Inventory
**Key Findings**:
_________________________________
_________________________________
_________________________________

### Priority Recommendations
**Immediate (0-30 days)**:
1. _________________________________
2. _________________________________

**Short-term (1-6 months)**:
1. _________________________________
2. _________________________________

**Long-term (6+ months)**:
1. _________________________________
2. _________________________________

### Follow-up Required
- [ ] Additional documentation review needed
- [ ] Follow-up interview with: ________________
- [ ] System demonstration required
- [ ] Other: ________________________________

### Auditor Confidence Level
- [ ] HIGH: Clear indicators, sufficient evidence
- [ ] MEDIUM: Some gaps in evidence, follow-up recommended
- [ ] LOW: Insufficient information, additional assessment needed

---

## üìã POST-ASSESSMENT CHECKLIST

### Before Leaving Client Site
- [ ] All evidence collection items attempted
- [ ] Field notes template completed
- [ ] Priority recommendations identified
- [ ] Follow-up actions documented
- [ ] Client questions addressed

### Within 24 Hours
- [ ] Formal assessment report drafted
- [ ] Evidence documentation organized
- [ ] Recommendation priorities confirmed
- [ ] Implementation timeline estimated
- [ ] Client presentation scheduled

### Quality Assurance
- [ ] Findings align with evidence collected
- [ ] Recommendations are specific and actionable
- [ ] Risk level justified by objective criteria
- [ ] Professional language maintained throughout
- [ ] Audit trail complete and traceable

---

**Total Assessment Time**: 22 minutes
**Report Generation**: +30 minutes
**Client Readiness**: Immediate