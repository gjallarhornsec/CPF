# üìã CPF INDICATOR 9.8 FIELD KIT
## Human-AI Team Dysfunction Assessment

---

## ‚ö° QUICK ASSESSMENT (5 minutes)

**Instructions**: Check YES/NO for each question based on observable evidence.

| # | Assessment Question | YES | NO | Notes |
|---|---------------------|-----|----|-------|
| 1 | **Structured Communication**: Do security staff use formal commands/syntax when interacting with AI tools (not conversational language)? | ‚òê | ‚òê | |
| 2 | **Written AI Policies**: Are there documented policies defining what decisions AI can/cannot make independently? | ‚òê | ‚òê | |
| 3 | **Override Procedures**: Do formal procedures exist for questioning/overriding AI recommendations? | ‚òê | ‚òê | |
| 4 | **AI Limitation Training**: Has security staff received specific training on AI system limitations and appropriate use? | ‚òê | ‚òê | |
| 5 | **Authentication Controls**: Are technical controls in place to prevent unauthorized systems from impersonating AI tools? | ‚òê | ‚òê | |
| 6 | **Decision Auditing**: Are human-AI security decisions reviewed monthly for appropriateness? | ‚òê | ‚òê | |
| 7 | **Information Sharing Controls**: Are there specific policies governing what sensitive information can be shared with AI systems? | ‚òê | ‚òê | |

**Quick Score**: ___/7 YES responses

---

## üìù EVIDENCE COLLECTION (10 minutes)

### Documents to Request
- [ ] **AI interaction protocols** or communication standards
- [ ] **Training materials** covering AI limitations and proper use
- [ ] **Decision authority matrix** (human vs. AI responsibilities)
- [ ] **Most recent audit** of human-AI security decisions
- [ ] **Information sharing policies** for AI systems
- [ ] **Authentication procedures** for AI tool verification

### Demonstrations to Request
- [ ] **"Show me how analysts communicate with AI during an alert"**
- [ ] **"Demonstrate the process when AI gives conflicting recommendations"**
- [ ] **"Walk through how you verify AI system authenticity"**
- [ ] **"Show recent examples of overriding AI recommendations"**

### System Checks to Perform
- [ ] **Review interaction logs** between humans and AI (last 30 days)
- [ ] **Check for conversational language** patterns in AI communications
- [ ] **Verify authentication mechanisms** for AI systems
- [ ] **Examine recent incident reports** for human-AI coordination issues

### Interview Targets
- [ ] **SOC Manager** - policies and procedures
- [ ] **2-3 SOC Analysts** - daily interaction patterns
- [ ] **Incident Response Lead** - decision-making during crises
- [ ] **Security Training Coordinator** - AI limitation education

---

## üéØ RAPID SCORING (2 minutes)

### Decision Tree

**Start Here** ‚ûú Count YES responses from Quick Assessment

**7 YES responses**
- All evidence shows structured protocols ‚ûú **GREEN (0)**

**5-6 YES responses**
- Most protocols exist but gaps identified ‚ûú **YELLOW (1)**

**3-4 YES responses**  
- Basic controls present but inconsistent ‚ûú **YELLOW (1)**

**0-2 YES responses**
- Minimal or no formal human-AI protocols ‚ûú **RED (2)**

### Override Criteria (Automatic RED regardless of score)
- [ ] Evidence of credentials shared with AI systems
- [ ] No authentication for AI tools
- [ ] Conversational communication is standard practice
- [ ] No training on AI limitations provided
- [ ] No decision authority documentation exists

**FINAL SCORE**: _________ (0=Green, 1=Yellow, 2=Red)

---

## üîß SOLUTION PRIORITIES (5 minutes)

### HIGH IMPACT - QUICK IMPLEMENTATION (0-30 days)

**Cost: LOW**
- [ ] **Create communication protocol** document requiring structured AI commands
- [ ] **Implement monthly decision audits** of human-AI interactions
- [ ] **Draft information sharing policy** for AI systems

### HIGH IMPACT - MEDIUM IMPLEMENTATION (30-90 days)

**Cost: MEDIUM**  
- [ ] **Deploy AI limitation training** for all security staff
- [ ] **Establish decision authority matrix** (human vs. AI responsibilities)
- [ ] **Create authentication procedures** for AI tool verification

### HIGH IMPACT - LONG-TERM (90+ days)

**Cost: HIGH**
- [ ] **Implement technical authentication controls** (certificates, API keys)
- [ ] **Deploy monitoring systems** for AI impersonation detection
- [ ] **Conduct quarterly coordination exercises** under stress conditions

### Dependencies
- **Training programs** require management approval and budget allocation
- **Technical controls** require IT/engineering resources and system integration
- **Policy changes** require legal review and change management process

---

## üí¨ CLIENT CONVERSATION SCRIPT (3 minutes)

### Opening Questions

**"How do your security analysts typically interact with AI tools during their daily work?"**
- *Follow-up*: "Can you show me some examples of recent conversations?"
- *Red flag*: Analysts describe AI tools like human teammates

**"When AI systems give unclear or conflicting advice, what's the standard procedure?"**
- *Follow-up*: "Who has authority to override AI recommendations?"
- *Red flag*: No clear escalation path or decision authority

**"What training have security staff received about AI system capabilities and limitations?"**
- *Follow-up*: "How often is this training updated or refreshed?"
- *Red flag*: Generic AI training without security-specific guidance

### Sensitive Topics (Professional Language)

**Information Sharing Concerns**:
- "Let's discuss your policies around what information can be shared with AI systems..."
- *Avoid*: "Are people sharing secrets with robots?"

**Anthropomorphization Issues**:
- "We're assessing the formal protocols for human-AI coordination..."
- *Avoid*: "Do your people think AI tools are human?"

**Trust Calibration**:
- "How does your organization ensure appropriate verification of AI recommendations?"
- *Avoid*: "Do you trust AI too much?"

### Closing Questions

**"Can you walk me through your most recent security incident that involved AI tool coordination?"**
- *Listen for*: Communication breakdowns, decision delays, unclear authority

**"What authentication measures prevent unauthorized systems from appearing as legitimate AI tools?"**
- *Listen for*: Technical controls, verification procedures, monitoring

---

## üìä FIELD NOTES TEMPLATE

### Client Information
- **Organization**: _________________________________
- **Assessment Date**: ____________________________  
- **Primary Contact**: _____________________________
- **Security Team Size**: ___________________________

### Key Findings
**Communication Patterns**:
- Structured protocols: **Yes/No** - Evidence: ________________
- Conversational usage: **Yes/No** - Examples: _______________

**Authority Structure**:
- Clear decision hierarchy: **Yes/No** - Documentation: ________
- Override procedures: **Yes/No** - Recent examples: __________

**Training & Awareness**:
- AI limitation training: **Yes/No** - Last conducted: __________
- Staff understanding level: **High/Medium/Low** - Notes: _____

**Technical Controls**:
- Authentication measures: **Present/Absent** - Details: _______
- Monitoring systems: **Active/Inactive** - Coverage: __________

### Risk Indicators Observed
- [ ] Emotional language about AI performance
- [ ] Credentials shared with AI systems  
- [ ] No verification of AI recommendations
- [ ] Confusion during AI coordination demos
- [ ] Missing authentication for AI tools

### Immediate Recommendations
1. **Priority 1**: ________________________________________
2. **Priority 2**: ________________________________________  
3. **Priority 3**: ________________________________________

### Follow-up Required
- [ ] Technical assessment of AI authentication controls
- [ ] Review of additional incident reports
- [ ] Interview with additional staff members
- [ ] Validation of training program effectiveness

**Auditor Signature**: _________________ **Date**: ____________